import streamlit as st
import io
from google.oauth2 import service_account
from google.cloud import speech
import google.generativeai as genai
from googleapiclient.discovery import build
from datetime import datetime

# ==========================================
# âš™ï¸ ì„¤ì • (í´ë¼ìš°ë“œ ë³´ì•ˆ ê¸ˆê³  ì‚¬ìš©)
# ==========================================
# secrets.tomlì— ì €ì¥ëœ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
# (ë‚˜ì¤‘ì— Streamlit ì‚¬ì´íŠ¸ì—ì„œ ì„¤ì •í•  ê²ë‹ˆë‹¤)
try:
    gcp_info = st.secrets["gcp_service_account"]
    GOOGLE_API_KEY = st.secrets["general"]["GOOGLE_API_KEY"]
    SHARED_DRIVE_ID = st.secrets["general"]["SHARED_DRIVE_ID"]
    AI_MODEL_NAME = 'gemini-2.0-flash'
except Exception:
    st.error("ğŸš¨ ë³´ì•ˆ ì„¤ì •(Secrets)ì´ ì•„ì§ ì•ˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤! Streamlit ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# ==========================================
# ğŸ› ï¸ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤
# ==========================================
def step1_transcribe(uploaded_file):
    with open("temp_meeting.mp3", "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    # â˜… ìˆ˜ì •ë¨: íŒŒì¼ì´ ì•„ë‹ˆë¼ 'ì •ë³´(info)'ë¡œ ì¸ì¦
    creds = service_account.Credentials.from_service_account_info(gcp_info)
    client = speech.SpeechClient(credentials=creds)

    with io.open("temp_meeting.mp3", "rb") as audio_file:
        content = audio_file.read()
    
    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED,
        sample_rate_hertz=16000,
        language_code="ko-KR",
        enable_automatic_punctuation=True,
        diarization_config=speech.SpeakerDiarizationConfig(
            enable_speaker_diarization=True,
            min_speaker_count=2,
            max_speaker_count=5,
        ),
    )
    
    operation = client.long_running_recognize(config=config, audio=audio)
    response = operation.result(timeout=600)

    transcript_text = ""
    result = response.results[-1]
    words_info = result.alternatives[0].words

    current_speaker = None
    sentence_buffer = []

    for word_info in words_info:
        speaker_tag = word_info.speaker_tag
        if current_speaker != speaker_tag:
            if current_speaker is not None:
                line = f"[í™”ì {current_speaker}]: {' '.join(sentence_buffer)}"
                transcript_text += line + "\n"
            current_speaker = speaker_tag
            sentence_buffer = []
        sentence_buffer.append(word_info.word)
    
    if sentence_buffer:
        line = f"[í™”ì {current_speaker}]: {' '.join(sentence_buffer)}"
        transcript_text += line + "\n"

    return transcript_text

def step2_summarize(transcript):
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel(AI_MODEL_NAME)
    prompt = f"""
    ë‹¹ì‹ ì€ KCH Globalì˜ ìœ ëŠ¥í•œ íšŒì˜ë¡ ì„œê¸°ì…ë‹ˆë‹¤.
    ì•„ë˜ ë…¹ì·¨ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    
    [ë…¹ì·¨ë¡]
    {transcript}
    
    [ì‘ì„± ì–‘ì‹]
    # ğŸ“… íšŒì˜ ìš”ì•½ ë³´ê³ ì„œ
    ## 1. í•µì‹¬ ì•ˆê±´
    ## 2. ì£¼ìš” ë…¼ì˜ ì‚¬í•­
    ## 3. ê²°ì • ì‚¬í•­
    ## 4. í–¥í›„ ê³„íš (ë‹´ë‹¹ì ì§€ì •)
    """
    response = model.generate_content(prompt)
    return response.text

def step3_save(summary, transcript):
    SCOPES = ['https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/documents']
    # â˜… ìˆ˜ì •ë¨: íŒŒì¼ì´ ì•„ë‹ˆë¼ 'ì •ë³´(info)'ë¡œ ì¸ì¦
    creds = service_account.Credentials.from_service_account_info(gcp_info, scopes=SCOPES)
    drive_service = build('drive', 'v3', credentials=creds)
    docs_service = build('docs', 'v1', credentials=creds)

    today = datetime.now().strftime("%Y-%m-%d %Hì‹œ%Më¶„")
    file_name = f"[AIíšŒì˜ë¡] {today} íšŒì˜ ê²°ê³¼"

    file_metadata = {
        'name': file_name,
        'mimeType': 'application/vnd.google-apps.document',
        'parents': [SHARED_DRIVE_ID]
    }
    file = drive_service.files().create(body=file_metadata, fields='id', supportsAllDrives=True).execute()
    doc_id = file.get('id')

    full_content = summary + "\n\n" + "-"*30 + "\n[ì°¸ê³ : ëŒ€í™” ì›ë³¸]\n" + transcript
    requests = [{'insertText': {'location': {'index': 1}, 'text': full_content}}]
    docs_service.documents().batchUpdate(documentId=doc_id, body={'requests': requests}).execute()
    return file_name

# ==========================================
# ğŸ–¥ï¸ í™”ë©´ êµ¬ì„±
# ==========================================
st.set_page_config(page_title="KCH Global AI íšŒì˜ë¡", page_icon="ğŸ™ï¸")
st.title("ğŸ™ï¸ KCH Global AI íšŒì˜ë¡ ìƒì„±ê¸°")
st.markdown("ì–¸ì œ ì–´ë””ì„œë‚˜ ë…¹ìŒ íŒŒì¼ë§Œ ì˜¬ë¦¬ì„¸ìš”. **AIê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.**")

uploaded_file = st.file_uploader("ë…¹ìŒ íŒŒì¼ ì—…ë¡œë“œ", type=["mp3", "wav", "m4a"])

if uploaded_file is not None:
    st.audio(uploaded_file, format='audio/mp3')
    if st.button("ğŸš€ íšŒì˜ë¡ ë§Œë“¤ê¸° ì‹œì‘"):
        with st.status("í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ì‘ì—… ì¤‘... (3~5ë¶„ ì†Œìš”)", expanded=True) as status:
            st.write("ğŸ§ 1ë‹¨ê³„: ë°›ì•„ì“°ê¸° ì¤‘...")
            transcript = step1_transcribe(uploaded_file)
            st.write("âœ… ë°›ì•„ì“°ê¸° ì™„ë£Œ!")
            
            st.write("ğŸ§  2ë‹¨ê³„: ìš”ì•½ ì¤‘...")
            summary = step2_summarize(transcript)
            st.write("âœ… ìš”ì•½ ì™„ë£Œ!")
            
            st.write("ğŸ’¾ 3ë‹¨ê³„: ì €ì¥ ì¤‘...")
            file_name = step3_save(summary, transcript)
            status.update(label="ğŸ‰ ì™„ë£Œ!", state="complete", expanded=False)

        st.success(f"'{file_name}' ì €ì¥ ì™„ë£Œ!")
        st.subheader("ë¯¸ë¦¬ë³´ê¸°")
        st.markdown(summary)